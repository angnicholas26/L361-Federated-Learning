{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angnicholas26/L361-Federated-Learning/blob/release/Lab1_Part1_Submission_Nicholas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTR0VzOm2B5d"
      },
      "source": [
        "# 0. Marking and Guidelines\n",
        "---\n",
        "***IMPORTANT***\n",
        "\n",
        "> The **attendance** and **active participation** in the **lab sessions** is **strongly recommended** and will be considered for grading.\n",
        ">\n",
        "> Save a copy of this notebook into your Drive before you start\n",
        ">\n",
        "> Please attempt all the **questions** marked for your **group** (Part II ✅ | Part III/MPhil ✅).\n",
        ">\n",
        "> Continue to **\"Part 2\"** after you are done with this **\"Part 1\"**.\n",
        ">\n",
        "> Please, provide your answers in a **new cell below the question cell**. You can make as many new cells as you need.\n",
        "\n",
        "Please submit a `.zip` file, containing both parts, consisting of:\n",
        "1. A text file with a **publicly** visible link to your notebooks in GitHub.\n",
        "2. A **downloaded copy** (`.ipynb`) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report---we will not be re-executing the code you used to produce the answers unless required.\n",
        "\n",
        "\n",
        "Feel free to attempt more in case you find yourself enjoying the material!\n",
        "If you have any questions, please ask them to the teaching assistants.\n",
        "Are you interested in knowing more about federated learning and related topics? Reach out to the teaching assistants for additional resources and ask more about the current research projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0eSNvNJFrjt"
      },
      "source": [
        "## 1. Introduction.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqp_FvE2B5f"
      },
      "source": [
        "Welcome to the first lab in our Federated Learning (FL) course.\n",
        "This series of labs intends to introduce you to the practice of constructing Federated Learning systems and understand their peculiarities compared to standard distributed learning systems.\n",
        "The most common form of FL, cross-device FL, is concerned with collaboratively training models on edge devices or \"clients\" using their private data sources without involving direct exchange of training samples.\n",
        "As such, FL may:\n",
        "- Reduce communication costs (compared to standard training approaches) by only sharing model parameters.\n",
        "- Distribute computation across an untapped pool of previously unused devices, such as personal smartphones, laptops, and/or workstations.\n",
        "- Partially preserve privacy by construction, as private data is never exchange and never leaves the private domain from which it is collected.\n",
        "\n",
        "A typical server-client FL system maintains a centralised server that synchronises training across iterations of rounds---the FL equivalent of epochs. A round generally proceeds as follows:\n",
        "1. At the beginning of a round, the server sends a copy of the federated model to each client.\n",
        "2. The clients then train their copy of the model on the private data they hold and send the post-training model updates to the server.\n",
        "3. The server then aggregates all the client models to form a single federated model update which it applies before the next round. The most common means of aggregation is Federated Averaging which merges model parameters via simple weighted averaging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhnE5-YklGrK"
      },
      "source": [
        "![fl_system_diagram](https://github.com/angnicholas26/L361-Federated-Learning/blob/release/assets/fl_system_diagram.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvDvRvsvN80X"
      },
      "source": [
        "To a more significant extent than other machine learning (ML) fields, FL has historically been driven by practical considerations rather than theoretical results.\n",
        "Some of the most critical practical concerns are related to the following:\n",
        "- Communication costs involved in training models in a highly distributed fashion across edge devices.\n",
        "- The underlying non-standardised hardware of clients within the federated network.\n",
        "- Highly divergent data distributions across clients.\n",
        "\n",
        "Such issues cannot be easily addressed, given FL's privacy and scale constraints, as persistent data on specific clients may not be trackable.\n",
        "In a worst-case setting, a single client may only participate in training once for cross-device FL with (tens of) millions of potential devices.\n",
        "\n",
        "In this lab session, we will use a realistic FL dataset to exemplify how such problems arise when ML is taken from a centralized or standard distributed setting to a federated one.\n",
        "Specific questions we will look at include the following:\n",
        "- How centralized ML can be modelled as FL with a single client and how to build a client abstraction capable of both.\n",
        "- How the federated and local optimization objectives of FL interact between each other and how different settings on the server/client side may impact convergence.\n",
        "\n",
        "To do so, we shall use the [Flower](https://flower.ai/) FL framework. The design of Flower is meant to allow for a fairly direct mapping between simulation and production FL scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zeAdW4nnSv5"
      },
      "source": [
        "![flower](https://github.com/angnicholas26/L361-Federated-Learning/blob/release/assets/flower.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m45tcB_gL0pw"
      },
      "source": [
        "## 2. Building a client.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvxdivaBODPZ"
      },
      "source": [
        "Rather than beginning with a complete standard centralised ML baseline, we will treat centralised ML as a particular case of FL--a single client holding the entire (centralised) dataset.\n",
        "\n",
        "For the rest of our lab sessions, a client shall represent an abstraction which:\n",
        "- Encapsulates the data available on a particular device involved in FL.\n",
        "- Trains a model it receives on that local (train) dataset.\n",
        "- Can test models it receives on its local (test) dataset.\n",
        "\n",
        "By the end of this section, you will have constructed such a client abstraction and trained a model on a full-scale dataset.\n",
        "Later work shall merely involve connecting multiple such clients in a federated network using the Flower framework.\n",
        "\n",
        "To begin, we will need to install some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XenFLb3FMF0s",
        "outputId": "459cb101-90ce-4b31-9c7e-b26523da5b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching\n",
            "  Cloning https://github.com/Iacob-Alexandru-Andrei/flower.git (to revision teaching) to /tmp/pip-req-build-5fsssk3p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Iacob-Alexandru-Andrei/flower.git /tmp/pip-req-build-5fsssk3p\n",
            "  Running command git checkout -b teaching --track origin/teaching\n",
            "  Switched to a new branch 'teaching'\n",
            "  Branch 'teaching' set up to track remote branch 'teaching' from 'origin'.\n",
            "  Resolved https://github.com/Iacob-Alexandru-Andrei/flower.git to commit 1c4fcc1d4a6e8022ddf6f94ebedef1b8e70e0fc4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting ray==2.6.3\n",
            "  Downloading ray-2.6.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (4.25.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (2.32.3)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (1.70.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from ray==2.6.3) (1.26.4)\n",
            "Collecting cryptography<42.0.0,>=41.0.2 (from flwr==1.7.0)\n",
            "  Downloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr==1.7.0)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting protobuf!=3.19.5,>=3.15.3 (from ray==2.6.3)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr==1.7.0)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr==1.7.0) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.6.3) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.6.3) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.6.3) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.6.3) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.6.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.6.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.6.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.6.3) (2025.1.31)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr==1.7.0) (2.22)\n",
            "Downloading ray-2.6.3-cp311-cp311-manylinux2014_x86_64.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flwr\n",
            "  Building wheel for flwr (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flwr: filename=flwr-1.7.0-py3-none-any.whl size=231058 sha256=1dd4026e953cc84734a76152ffe91c9b02b5eb974fb66c6c7a052ce811b12377\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wltw5dkk/wheels/f7/84/4f/213c2e8fddee802ed14165c6377a4dc5ac98d3c6a2e00fda49\n",
            "Successfully built flwr\n",
            "Installing collected packages: pycryptodome, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, iterators, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cryptography, nvidia-cusolver-cu12, flwr, ray\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cryptography-41.0.7 flwr-1.7.0 iterators-0.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 protobuf-3.20.3 pycryptodome-3.21.0 ray-2.6.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "96dfeb9fc5ad4c109a49e946efb6af47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "# `pip` could produce some errors. Do not worry about them.\n",
        "# The execution has been verified; it's working anyway.\n",
        "! pip install --quiet --upgrade \"pip\"\n",
        "! pip install --quiet matplotlib tqdm seaborn\n",
        "! pip install git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching \\\n",
        "    torch torchvision ray==\"2.6.3\"\n",
        "# The following is just needed to show the folder tree\n",
        "! apt-get install -qq tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiW6ObflQhSi"
      },
      "source": [
        "Our first client abstraction shall be as simple as possible and will require adjustment to match the structure that the framework expects.\n",
        "However, it shall be conceptually identical and require only light API changes.\n",
        "\n",
        "In the next few cells, we will just import the required modules and instantiate some useful function and objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eO7LWfu4RL7a"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "from typing import Any\n",
        "from logging import INFO\n",
        "from abc import abstractmethod\n",
        "from collections.abc import Sequence, Callable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from PIL.Image import Image as ImageType\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from enum import IntEnum\n",
        "from flwr.common import log\n",
        "\n",
        "\n",
        "# Add new seeds here for easy autocomplete\n",
        "class Seeds(IntEnum):\n",
        "    \"\"\"Seeds for reproducibility.\"\"\"\n",
        "\n",
        "    DEFAULT = 1337\n",
        "\n",
        "\n",
        "np.random.seed(Seeds.DEFAULT)\n",
        "random.seed(Seeds.DEFAULT)\n",
        "torch.manual_seed(Seeds.DEFAULT)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "PathType = Path | str | None\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    \"\"\"Get the device (cuda, mps, cpu).\"\"\"\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        device = \"mps\"\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fa-jqHRaQ1C2"
      },
      "outputs": [],
      "source": [
        "class SimpleClient:\n",
        "    \"\"\"Simple client class for federated learning.\"\"\"\n",
        "\n",
        "    def __init__(self, cid: int, partition_dir: Path) -> None:\n",
        "        \"\"\"\n",
        "        Init the client with its unique id and the directory from which it loads data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            cid (int): Unique client id for a client used to map it to its data\n",
        "                partition\n",
        "            partition_dir (Path): The directory containing data for each\n",
        "                client/client id\n",
        "        \"\"\"\n",
        "        self.cid: str = str(cid)\n",
        "        self.partition_dir: Path = partition_dir\n",
        "        self.device: str = get_device()\n",
        "\n",
        "    def fit(self, net: Module, config: dict[str, Any]) -> tuple[Module, int, dict]:\n",
        "        \"\"\"Receive and train a model on the local client data.\n",
        "\n",
        "        It uses parameters from the config dict\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            net (Module): Pytorch model\n",
        "            config (Dict[str, Any]): Dictionary describing the training parameters\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            Tuple[Module, int, dict]: Returns the updated model, the size of the local\n",
        "                dataset and (optionally) other metrics\n",
        "        \"\"\"\n",
        "        net = deepcopy(net)\n",
        "        net.to(self.device)\n",
        "        train_loader: DataLoader = self._create_data_loader(config, name=\"train\")\n",
        "        self._train(net, train_loader=train_loader, config=config)\n",
        "        return net, len(train_loader), {}\n",
        "\n",
        "    def evaluate(self, net: Module, config: dict[str, Any]) -> tuple[float, int, dict]:\n",
        "        \"\"\"Receive and test a model on the local client data.\n",
        "\n",
        "        It uses parameters from the config dict.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            net (Module): Pytorch model\n",
        "            config (Dict[str, Any]): Dictionary describing the test parameters\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            Tuple[float, int, dict]: Returns the loss accumulate during test, the size\n",
        "                of the local dataset and other metrics such as accuracy\n",
        "        \"\"\"\n",
        "        net = deepcopy(net)\n",
        "        net.to(self.device)\n",
        "        test_loader: DataLoader = self._create_data_loader(config, name=\"test\")\n",
        "        loss, accuracy = self._test(net, test_loader=test_loader, config=config)\n",
        "        return loss, len(test_loader), {\"local_accuracy\": accuracy}\n",
        "\n",
        "    def _create_data_loader(self, config: dict[str, Any], name: str) -> DataLoader:\n",
        "        \"\"\"Create the data loader using the specified config parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            config (Dict[str, any]): Dictionary containing dataloader and dataset\n",
        "                parameters\n",
        "            name (str): Load the training or testing set for the client\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            DataLoader: A pytorch dataloader iterable for training/testing\n",
        "        \"\"\"\n",
        "        batch_size: int = config[\"batch_size\"]\n",
        "        num_workers: int = config[\"num_workers\"]\n",
        "        dataset = self._load_dataset(name, config)\n",
        "        return DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=name != \"test\",\n",
        "        )\n",
        "\n",
        "    # All methods from here on are task-specific\n",
        "    # And need to be implemented in a later stage\n",
        "    @abstractmethod\n",
        "    def _load_dataset(self, name: str, config: dict[str, Any]) -> Dataset:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _train(\n",
        "        self, net: Module, train_loader: DataLoader, config: dict[str, Any]\n",
        "    ) -> float:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _test(\n",
        "        self, net: Module, test_loader: DataLoader, config: dict[str, Any]\n",
        "    ) -> tuple[float, float]:\n",
        "        pass\n",
        "\n",
        "\n",
        "def fit_client_seeded(\n",
        "    client: SimpleClient,\n",
        "    params: Module,\n",
        "    conf: dict[str, Any],\n",
        "    seed: Seeds = Seeds.DEFAULT,\n",
        "    **kwargs: Any,\n",
        ") -> tuple[Module, int, dict]:\n",
        "    \"\"\"Wrap to always seed client training.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    return client.fit(params, conf, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfcgBtrhVtAo"
      },
      "source": [
        "This client covers all the previously stated functionality; it can:\n",
        "- Associate a given dataset/dataset partition to itself based on the `data_dir` and `cid` combination\n",
        "- Train any model provided to its `fit` function on the local training set\n",
        "- Evaluate any model on the local test set.\n",
        "\n",
        "To get this client training, we need to fill in the necessary gaps: the dataset loading procedure, the model, the training procedure and the testing procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPf2tk1P2B5l"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 1 (Part II ✅ | Part III/MPhil ✅):**\n",
        "\n",
        "(These are meant to be conceptual questions. You should provide written answers for these. **No more than 3 sentences each**. **No code** is needed)\n",
        "\n",
        "1. What is the time complexity of transmitting models to-and-from the client?\n",
        "2. Give at least two examples of machine learning and data science techniques you cannot apply since data is kept private on single clients.\n",
        "\n",
        "---\n",
        "\n",
        "**Response:**\n",
        "\n",
        "1. Since the client requires the entire model to be provided in order to perform training/evaluation, the time complexity of communication will be linear in the size of the model. This is represented by the ``deepcopy`` operation which is linear in the size of the model.\n",
        "\n",
        "2.\n",
        "- You can't perform exploratory data analysis (EDA), since EDA is by nature an open-ended problem that involves looking at the data and trying to find patterns, which you cannot do if you do not have access to the data\n",
        "\n",
        "- It will be very difficult to build a data normalisation pipeline up-front. Without access to the data, it would be hard to predict the different anomalies and how to resolve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zocsNC84Wk_l"
      },
      "source": [
        "## 3. Loading the federated dataset.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NhUGGSrfkrJ"
      },
      "source": [
        "We have chosen to use the FEMNIST dataset originally published in [LEAF](https://arxiv.org/abs/1812.01097) for all of our labs due to its naturally heterogeneous properties.\n",
        "Most FL examples rely on a centralised dataset such as CIFAR-10, which is artificially partitioned to fit an FL context.\n",
        "FEMNIST is an image dataset with 28×28 greyscale images split into 62 distinct classes.\n",
        "We shall initially treat it as a centralised dataset.\n",
        "However, its federated nature will be helpful later on.\n",
        "\n",
        "It is, by default, partitioned based on the creator of a specific image, thus modelling a realistic data-generation scenario.\n",
        "A complete list of its statistics may be found on the [LEAF Website](https://leaf.cmu.edu/).\n",
        "\n",
        "We will extensively discuss federated datasets during the next lab.\n",
        "In this session, we will skip such discussions to spend more time on \"federating\" a centralised model.\n",
        "The following few cells will provide all the relevant functions and objects to load the FEMNIST dataset.\n",
        "Also, some **optional** insights will be given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBkp-2M4H9SZ",
        "outputId": "b7af6da4-797f-463f-a3bd-5d505ec161ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 21:20:18--  https://github.com/camlsys/L361-Federated-Learning/raw/refs/heads/release/labs/femnist.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/camlsys/L361-Federated-Learning/refs/heads/release/labs/femnist.tar.gz [following]\n",
            "--2025-02-10 21:20:18--  https://media.githubusercontent.com/media/camlsys/L361-Federated-Learning/refs/heads/release/labs/femnist.tar.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 222525567 (212M) [application/octet-stream]\n",
            "Saving to: ‘femnist.tar.gz’\n",
            "\n",
            "femnist.tar.gz      100%[===================>] 212.22M   154MB/s    in 1.4s    \n",
            "\n",
            "2025-02-10 21:20:22 (154 MB/s) - ‘femnist.tar.gz’ saved [222525567/222525567]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/camlsys/L361-Federated-Learning/raw/refs/heads/release/labs/femnist.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xDpybqNURmr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a92e72-3a0f-49ae-ff71-a1366650f6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2025-02-10 21:28:05,861 | <ipython-input-5-4c2a8b254464>:12 | Dataset extracted in /content/femnist\n",
            "INFO:flwr:Dataset extracted in /content/femnist\n"
          ]
        }
      ],
      "source": [
        "home_dir = Path.cwd()\n",
        "dataset_dir: Path = home_dir / \"femnist\"\n",
        "data_dir: Path = dataset_dir / \"data\"\n",
        "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
        "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
        "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
        "\n",
        "# Decompress dataset\n",
        "if not dataset_dir.exists():\n",
        "    with tarfile.open(home_dir / \"femnist.tar.gz\", \"r:gz\") as tar:\n",
        "        tar.extractall(path=home_dir)\n",
        "    log(INFO, \"Dataset extracted in %s\", dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiMa5eYORmr6"
      },
      "source": [
        "You can **optionally** execute the following cell to look at the folder tree induced by unzipping the compressed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FR92a5uyRmr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923bbeb9-a2c0-4fd5-bde0-3fa2c0fcb471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m./\u001b[0m\n",
            "├── \u001b[01;34mfemnist\u001b[0m\n",
            "│   ├── \u001b[01;34mclient_data_mappings\u001b[0m\n",
            "│   │   ├── \u001b[01;34mcentralized\u001b[0m\n",
            "│   │   └── \u001b[01;34mfed_natural\u001b[0m\n",
            "│   └── \u001b[01;34mdata\u001b[0m\n",
            "│       ├── \u001b[01;34mtest\u001b[0m\n",
            "│       ├── \u001b[01;34mtrain\u001b[0m\n",
            "│       └── \u001b[01;34mval\u001b[0m\n",
            "└── \u001b[01;34msample_data\u001b[0m\n",
            "\n",
            "9 directories\n"
          ]
        }
      ],
      "source": [
        "# Showing resulting folder tree\n",
        "! tree -dC -L 3 ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-70unJnRmr6"
      },
      "source": [
        "For those of you gifted with eager curiosity, you can find the [Jupyter notebook](https://drive.google.com/file/d/1-I0uPPzm1ONlLD-u4XCFGqUgz6KPJzDe/view?usp=share_link) used to create this dataset from the version publicly available from TensorFlow Federated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moqCXNdjRmr6"
      },
      "source": [
        "We have thus downloaded two different partitions of the FEMNIST dataset.\n",
        "We will first use the \"centralised\" partition, in which we have distributed all the samples to one client. The second is the \"natural\" partition, in which the images/samples have been divided by writers.\n",
        "\n",
        "Samples have been collected once in the `data` folder, composed of three subfolders- `train`, `test` and `val`-, each containing one file for each sample in those sets. Then, it is the `client_data_mappings` folder that contains the relevant structure, composed of subfolders and `.csv` files, each describing the partitions. In fact, for each client, a `train.csv` and a `test.csv` are provided, containing a row for each sample in the client set that reports `client_id`, `sample_path`, `sample_id`, `sample_label`. It is worth mentioning that while train and test sets have been composed of the same clients, the validation set has been composed of different clients. That is why the only `val.csv` contained in this folder structure is under the centralised partition.\n",
        "\n",
        "The number of clients composing the train and test sets is 3230, while the number of clients composing the `val` set is 170. The splitting has been chosen to make the validation step quick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muCGzjZ9qWoj"
      },
      "source": [
        "## 4. Centralized ML\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHmJIyUsWi7O"
      },
      "source": [
        "We will begin with our centralised “partition”. We then use the entire `train.csv` and `test.csv` contained in `client_data_mappings/centralized/0`. For a large section of FL tasks, the centralised baseline represents the upper limit of performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wgSy30y6n8hR"
      },
      "outputs": [],
      "source": [
        "class FEMNIST(Dataset):\n",
        "    \"\"\"Dataset class for the FEMNIST dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        mapping: Path,\n",
        "        data_dir: Path = data_dir,\n",
        "        name: str = \"train\",\n",
        "        transform: Callable[[ImageType], Any] | None = None,\n",
        "        target_transform: Callable[[int], Any] | None = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialise the FEMNIST dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            mapping (Path): path to the mapping folder containing the .csv files.\n",
        "            data_dir (Path): path to the dataset folder. Defaults to data_dir.\n",
        "            name (str): name of the dataset to load, train or test.\n",
        "            transform (Callable[[ImageType], Any] | None, optional): transform function\n",
        "                to be applied to the ImageType object. Defaults to None.\n",
        "            target_transform (Callable[[int], Any] | None, optional): transform function\n",
        "                to be applied to the label. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.mapping = mapping\n",
        "        self.name = name\n",
        "\n",
        "        self.data: Sequence[tuple[str, int]] = self._load_dataset()\n",
        "        self.transform: Callable[[ImageType], Any] | None = transform\n",
        "        self.target_transform: Callable[[int], Any] | None = target_transform\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[Any, Any]:  # noqa: ANN001\n",
        "        \"\"\"Get a sample respecting PyTorch directives.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            index : index of the sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            Tuple[Any, Any]: couple (sample, label).\n",
        "        \"\"\"\n",
        "        sample_path, label = self.data[index]\n",
        "\n",
        "        # Convert to the full path\n",
        "        full_sample_path: Path = self.data_dir / self.name / sample_path\n",
        "\n",
        "        img: ImageType = Image.open(full_sample_path).convert(\"L\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Get the length of the dataset as the number of samples.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            int: the length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def _load_dataset(self) -> Sequence[tuple[str, int]]:\n",
        "        \"\"\"Load the paths and labels of the partition.\n",
        "\n",
        "        Preprocess the dataset for faster future loading,\n",
        "        if opened for the first time.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "            ValueError: raised if the mapping file does not exist.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            Sequence[tuple[str, int]]: partition asked as a sequence of\n",
        "                couples (path_to_file, label)\n",
        "        \"\"\"\n",
        "        preprocessed_path: Path = (self.mapping / self.name).with_suffix(\".pt\")\n",
        "        if preprocessed_path.exists():\n",
        "            return torch.load(preprocessed_path)\n",
        "        else:\n",
        "            csv_path = (self.mapping / self.name).with_suffix(\".csv\")\n",
        "            if not csv_path.exists():\n",
        "                raise ValueError(f\"Required files do not exist, path: {csv_path}\")\n",
        "            with open(csv_path, encoding=\"utf-8\") as csv_file:\n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                # Ignore header\n",
        "                next(csv_reader)\n",
        "\n",
        "                # Extract the samples and the labels\n",
        "                partition: Sequence[tuple[str, int]] = [\n",
        "                    (sample_path, int(label_id))\n",
        "                    for _, sample_path, _, label_id in csv_reader\n",
        "                ]\n",
        "\n",
        "                # Save for future loading\n",
        "                torch.save(partition, preprocessed_path)\n",
        "                return partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_o3YKN3UpNmg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "2391e0f9-66b7-4572-dc4d-f14ea39134f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmUlEQVR4AWP8z4AbMOGWYmAYRpIsKP7894+BCclzjMgh9J8RRSkDiiTDtR3MPspISv7Dwd9fqcw2wpr//8FFGOCs3//nsGz//1Dwxf+/MDFkSR+z//83mfyBy/1HSP75t4UvvERk4/8/MI3/EQ5n/u+95Y/oSb//zHAnI7v2H0glmIBKI0syoIUBmj/hBkIYCDvRJEDcYSQJALJrd6RY7PX/AAAAAElFTkSuQmCC\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKK5LxV4vm0XUoNNtEsEnktpLtrjUbkwQoiEDAIBLMSeg6Dk10GkXsmo6NY301u1tLcW8crwMcmMsoJU/TOKu0VDPaW100bXFvFKYm3xmRA2xvUZ6GpqKKKKK//2Q==\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset: FEMNIST = FEMNIST(mapping=centralized_mapping, data_dir=data_dir, name=\"train\")\n",
        "\n",
        "# Show random value\n",
        "img, _ = dataset[4]\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp12GiFVaYXo"
      },
      "source": [
        "Given this fully “centralised” dataset, the client abstraction only requires functions implementing data loading, model training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "URtFwYX0iNSR"
      },
      "outputs": [],
      "source": [
        "# Load with appropriate transforms\n",
        "def to_tensor_transform(p: Any) -> torch.Tensor:\n",
        "    \"\"\"Transform the object given to a PyTorch Tensor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        p (Any): object to transform.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        torch.Tensor: resulting PyTorch Tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor(p)\n",
        "\n",
        "\n",
        "def load_femnist_dataset(mapping: Path, name: str) -> Dataset:\n",
        "    \"\"\"Load the FEMNIST dataset given the mapping .csv file.\n",
        "\n",
        "    The relevant transforms are automatically applied.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        mapping (Path): path to the mapping .csv file chosen.\n",
        "        name (str): name of the dataset to load, train or test.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Dataset: FEMNIST dataset object, ready-to-use.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    return FEMNIST(\n",
        "        mapping=mapping,\n",
        "        name=name,\n",
        "        data_dir=data_dir,\n",
        "        transform=transform,\n",
        "        target_transform=to_tensor_transform,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnFffXP0FBUB"
      },
      "source": [
        "Partitions in FL datasets may vary in data-per-client by orders of magnitude. While in the “centralised” case, one client holds $O(N)$ samples with $N \\approx 80,5263$, a natural partitioning by image creator ID will result in each client holding $O(m)$ samples with $m \\approx 226$.\n",
        "\n",
        "This points to a more fundamental issue in FL, the unbalanced size of local datasets across clients. Several questions arise when considering the potential orders-of-magnitude gap between clients:\n",
        "1. How do we set the amount of training each client does per round?\n",
        "2. How should we balance the contribution of several clients, given differences in dataset size?\n",
        "\n",
        "The original paper introducing FL, [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629), and the Federated Averaging algorithm we shall use today answers the questions in the following ways:\n",
        "1. Set a number of local epochs homogenous across clients; each client iterates over their entire dataset for the provided number of epochs.\n",
        "2. When combining the models of different clients, weigh them by the size of the local dataset of the clients.\n",
        "\n",
        "For now, we shall choose to keep the local epoch design and place an absolute upper limit on the number of batches seen during training/testing due to the computational constraints of training the centralised client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z2ItflSDFBUC"
      },
      "outputs": [],
      "source": [
        "max_train_batches_per_epoch: int = 100\n",
        "max_test_batches_per_epoch: int = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOXB6EkiFBUC"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 2 (Part II ✅ | Part III/MPhil ✅):**\n",
        "\n",
        "(This is meant to be a conceptual question. You should provide written answers for this. **No more than 3 sentences**. **No code** is needed)\n",
        "\n",
        "Consider the three following scenarios for a population of five clients and identify how weighing client updates by the number of samples compares to unweighted averaging:\n",
        "1. Clients have the same number of samples\n",
        "2. One client has 50% of the samples, and they are of high quality (e.g., coherent English text)\n",
        "3. One client has 50% of the samples, and they are of very low quality (e.g., incomprehensible mashup of letters)\n",
        "\n",
        "**Answer:**\n",
        "Case 1 - both weighted and unweighted have the same performance, case 2 - weighted outperforms, case 3 - unweighted outperforms.\n",
        "\n",
        "In the weighted case, a client that has a large number of samples will exert a much greater impact on the overall performance of the model than the unweighted case, so the model's performance depends more strongly on the quality of data held by the majority client.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3 (Part III/MPhil ✅):**\n",
        "\n",
        "(This is meant to be a conceptual question. You should provide written answers for this. **No more than 3 sentences**. **No code** is needed)\n",
        "1. In the previous three scenarios, what would happen if we set a homogenous number of training steps for all clients instead of epochs? I.e., clients loop over their dataset until the precise moment when they have processed N total samples.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YYcbCW4FBUC"
      },
      "source": [
        "While FL may use any training function that centralised ML can, there is an extra consideration regarding how much information a client may leak regarding their local dataset. For example, the model may contain enough data to recreate local samples perfectly under the right circumstances unless defensive measures are employed.\n",
        "\n",
        "Later labs and lectures will explore several methods for mitigating such concerns. Until then, the three following pieces of information are currently considered acceptable from a privacy standpoint:\n",
        "1. Model weights.\n",
        "2. The size of the local dataset.\n",
        "3. Cumulative loss values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fyX641rBkYiv"
      },
      "outputs": [],
      "source": [
        "def train_femnist(\n",
        "    net: Module,\n",
        "    train_loader: DataLoader,\n",
        "    epochs: int,\n",
        "    device: str,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: Module,\n",
        ") -> float:\n",
        "    \"\"\"Trains the network on the training set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        net (Module): generic module object describing the network to train.\n",
        "        train_loader (DataLoader): dataloader to iterate during the training.\n",
        "        epochs (int): number of epochs of training.\n",
        "        device (str): device name onto which perform the computation.\n",
        "        optimizer (torch.optim.Optimizer): optimizer object.\n",
        "        criterion (Module): generic module describing the loss function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        float: the final epoch mean train loss.\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    running_loss, total = 0.0, 0\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        running_loss = 0.0\n",
        "        total = 0\n",
        "        for batch_cnt, (data, labels) in enumerate(train_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            _outputs = net(data)\n",
        "            loss = criterion(net(data), labels)\n",
        "            running_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Break if we have exceeded the upper limit\n",
        "            # On training batches for a given round\n",
        "            # Simulate enumerate counting for train/test parity\n",
        "            if batch_cnt > max_train_batches_per_epoch:\n",
        "                break\n",
        "    return running_loss / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TmFKmWHLkl-Z"
      },
      "outputs": [],
      "source": [
        "def test_femnist(\n",
        "    net: Module,\n",
        "    test_loader: DataLoader,\n",
        "    device: str,\n",
        "    criterion: Module,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"Validate the network on a test set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        net (Module): generic module object describing the network to test.\n",
        "        test_loader (DataLoader): dataloader to iterate during the testing.\n",
        "        device (str):  device name onto which perform the computation.\n",
        "        criterion (Module): generic module describing the loss function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        tuple[float, float]: couple of average test loss and average accuracy on the\n",
        "            test set.\n",
        "    \"\"\"\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(\n",
        "            tqdm(test_loader, total=min(max_test_batches_per_epoch, len(test_loader)))\n",
        "        ):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = net(data)\n",
        "\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Break if we have exceed the upper limit\n",
        "            # On testing batches\n",
        "            if i > max_test_batches_per_epoch:\n",
        "                break\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LWzf1Z9Q40ia"
      },
      "outputs": [],
      "source": [
        "class CompleteSimpleClient(SimpleClient):\n",
        "    \"\"\"Complete SimpleClient for FEMNIST dataset.\"\"\"\n",
        "\n",
        "    def _load_dataset(self, name: str, config: dict[str, Any]) -> Dataset:\n",
        "        full_file: Path = self.partition_dir / self.cid\n",
        "        return load_femnist_dataset(mapping=full_file, name=name)\n",
        "\n",
        "    def _train(\n",
        "        self, net: Module, train_loader: DataLoader, config: dict[str, Any]\n",
        "    ) -> float:\n",
        "        # Notice the usage of the config dict to obtain training\n",
        "        # parameters for a given client\n",
        "        return train_femnist(\n",
        "            net=net,\n",
        "            train_loader=train_loader,\n",
        "            epochs=config[\"epochs\"],\n",
        "            device=self.device,\n",
        "            optimizer=torch.optim.AdamW(\n",
        "                net.parameters(),\n",
        "                lr=config[\"client_learning_rate\"],\n",
        "                weight_decay=config[\"weight_decay\"],\n",
        "            ),\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "        )\n",
        "\n",
        "    def _test(\n",
        "        self, net: Module, test_loader: DataLoader, config: dict[str, Any]\n",
        "    ) -> tuple[float, float]:\n",
        "        return test_femnist(\n",
        "            net=net,\n",
        "            test_loader=test_loader,\n",
        "            device=self.device,\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asc5DtAN5zXu"
      },
      "source": [
        "This client can now encapsulate any partition of the FEMNIST dataset and train and test on it. To put it into action, we shall define a small CNN taken from the [60 minute PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network) and the training/testing configurations used by the fit/evaluate functions of the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9khkMcJ142cX"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"Simple CNN for FEMNIST dataset.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 62)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass of the network.\"\"\"\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTGpzOTdFBUC"
      },
      "source": [
        "The following global variables will be set to standardise training and testing across experiments. This means fixing:\n",
        "- The starting initialised model.\n",
        "- Local train/test parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wmsbBUGyFBUC"
      },
      "outputs": [],
      "source": [
        "# All experiments will have the same initialisation.\n",
        "# All differences in performance will come from training\n",
        "\n",
        "\n",
        "def get_network_generator() -> Callable[[], Net]:\n",
        "    \"\"\"Get function to generate a new untrained network.\"\"\"\n",
        "    untrained_net: Net = Net()\n",
        "\n",
        "    def generated_net() -> Net:\n",
        "        return deepcopy(untrained_net)\n",
        "\n",
        "    return generated_net\n",
        "\n",
        "\n",
        "network_generator = get_network_generator()\n",
        "\n",
        "centralized_train_config: dict[str, Any] = {\n",
        "    \"epochs\": 5,\n",
        "    \"batch_size\": 32,\n",
        "    \"client_learning_rate\": 0.01,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"num_workers\": 0,\n",
        "}\n",
        "\n",
        "test_config: dict[str, Any] = {\n",
        "    \"batch_size\": 32,\n",
        "    \"num_workers\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0cV3STZ16vYx"
      },
      "outputs": [],
      "source": [
        "# Instantiate the centralised client and model\n",
        "centralized_client = CompleteSimpleClient(cid=0, partition_dir=centralized_partition)\n",
        "centralized_net = network_generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UOEKuiCQ6KVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827a28d6-bbe3-4a8e-cb3a-c56920c1baf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-bdbb6e9cdb06>:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(preprocessed_path)\n",
            "100%|██████████| 5/5 [00:24<00:00,  4.90s/it]\n",
            "INFO flwr 2025-02-10 21:28:35,192 | <ipython-input-17-146a1ebc5b1e>:5 | Fit results = [20189, {}]\n",
            "INFO:flwr:Fit results = [20189, {}]\n"
          ]
        }
      ],
      "source": [
        "# Fit the centralised model\n",
        "centralized_net, *rest = fit_client_seeded(\n",
        "    centralized_client, params=centralized_net, conf=centralized_train_config\n",
        ")\n",
        "log(INFO, \"Fit results = %s\", rest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Jj4rAKSx6s8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc98cee1-4e2a-4f72-fe28-c64ed0c6c5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "101it [00:01, 72.18it/s]                        \n",
            "INFO flwr 2025-02-10 21:28:36,880 | <ipython-input-18-b2bf143bc452>:3 | Test results = (175.93068766593933, 2329, {'local_accuracy': 0.5193014705882353})\n",
            "INFO:flwr:Test results = (175.93068766593933, 2329, {'local_accuracy': 0.5193014705882353})\n"
          ]
        }
      ],
      "source": [
        "# Test the trained centralised model\n",
        "result = centralized_client.evaluate(net=centralized_net, config=test_config)\n",
        "log(INFO, \"Test results = %s\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INODwg_gVyY2"
      },
      "source": [
        "---\n",
        "\n",
        "**Question 4 (Part II ✅ | Part III/MPhil ✅):**\n",
        "\n",
        "(This is meant to be a conceptual question. You should provide written answers for this. **No more than 3 sentences**. **No code** is needed)\n",
        "\n",
        "1. Read about [data-parallelism](https://d2l.ai/chapter_computational-performance/multiple-gpus.html), if we were to train the centralized model in a data-parallel fashion how often would we need to communicate between the model replicas?\n",
        "\n",
        "Each time we finish training a mini-batch, we have to communicate the local gradient updates to the other replicas so that all the different updates can be aggregated (using AllReduce), and so all replicas can receive the updated gradients for the next mini-batch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RxdrTSpuXEE"
      },
      "source": [
        "## END OF PART I:\n",
        "\n",
        "Continue to the part 2 notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE4qbA0rQN69"
      },
      "source": [
        "(c) 2025 Alexandru-Andrei Iacob, Lorenzo Sani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I_xIzshGsrIu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "labs-MC7Y7X2c-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}