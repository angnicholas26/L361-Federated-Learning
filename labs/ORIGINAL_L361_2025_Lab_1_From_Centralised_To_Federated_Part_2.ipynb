{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTR0VzOm2B5d"
   },
   "source": [
    "# 0. Marking and Guidelines\n",
    "---\n",
    "***IMPORTANT***\n",
    "\n",
    "> The **attendance** and **active participation** in the **lab sessions** is **strongly recommended** and will be considered for grading.\n",
    ">\n",
    "> Save a copy of this notebook into your Drive before you start\n",
    "> \n",
    "> Please attempt all the **questions** marked for your **group** (Part II ✅ | Part III/MPhil ✅).\n",
    "> \n",
    "> Please, provide your answers in a **new cell below the question cell**. You can make as many new cells as you need.\n",
    "\n",
    "Please submit a `.zip` file, containing both parts, consisting of:\n",
    "1. A text file with a **publicly** visible link to your notebooks in GitHub.\n",
    "2. A **downloaded copy** (`.ipynb`) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report---we will not be re-executing the code you used to produce the answers unless required.\n",
    "\n",
    "\n",
    "Feel free to attempt more in case you find yourself enjoying the material!\n",
    "If you have any questions, please ask them to the teaching assistants.\n",
    "Are you interested in knowing more about federated learning and related topics? Reach out to the teaching assistants for additional resources and ask more about the current research projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1P9epXi-IZ0"
   },
   "source": [
    "## Dependencies\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTM8comw-IZ2"
   },
   "outputs": [],
   "source": [
    "# `pip` could produce some errors. Do not worry about them.\n",
    "# The execution has been verified; it's working anyway.\n",
    "! pip install --quiet --upgrade \"pip\"\n",
    "! pip install --quiet matplotlib tqdm seaborn\n",
    "! pip install git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching \\\n",
    "    torch torchvision ray==\"2.6.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zl2hUa1-IZ2"
   },
   "source": [
    "### Imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7xM_SyD-IZ4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from typing import Any\n",
    "from logging import INFO\n",
    "from collections import defaultdict, OrderedDict\n",
    "from collections.abc import Sequence, Callable\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from enum import IntEnum\n",
    "import flwr\n",
    "from flwr.server import History, ServerConfig\n",
    "from flwr.server.strategy import FedAvgM as FedAvg, Strategy\n",
    "from flwr.common import log, NDArrays, Scalar, Parameters, ndarrays_to_parameters\n",
    "from flwr.client.client import Client\n",
    "\n",
    "from labs.common.client_utils import (\n",
    "    Net,\n",
    "    load_femnist_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    train_femnist,\n",
    "    test_femnist,\n",
    "    save_history,\n",
    ")\n",
    "\n",
    "\n",
    "# Add new seeds here for easy autocomplete\n",
    "class Seeds(IntEnum):\n",
    "    \"\"\"Seeds for reproducibility.\"\"\"\n",
    "\n",
    "    DEFAULT = 1337\n",
    "\n",
    "\n",
    "np.random.seed(Seeds.DEFAULT)\n",
    "random.seed(Seeds.DEFAULT)\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "PathType = Path | str | None\n",
    "\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Get the device (cuda, mps, cpu).\"\"\"\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "341p1c5o-IZ4"
   },
   "outputs": [],
   "source": [
    "home_dir = Path.cwd()\n",
    "dataset_dir: Path = home_dir / \"femnist\"\n",
    "data_dir: Path = dataset_dir / \"data\"\n",
    "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
    "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
    "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
    "\n",
    "# Decompress dataset\n",
    "if not dataset_dir.exists():\n",
    "    with tarfile.open(home_dir / \"femnist.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=home_dir)\n",
    "    log(INFO, \"Dataset extracted in %s\", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bLMKg-y-IZ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Q6qaJbjac5"
   },
   "source": [
    "## 5. Building a Flower FL client.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUEPLGXCxhDe"
   },
   "source": [
    "Our first client abstraction shall be as simple as possible and will require adjustment to match the structure that the flower framework expects. However, it shall be conceptually identical and require only light API changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh6ebOepFBUC"
   },
   "source": [
    "Moving from centralised ML to server-client FL requires us to provide a means of communication between the respective server and clients. The Flower Framework is ML-framework agnostic and allows various means of transmitting model parameters in the federated network. Since you may have limited resources in these labs, we will only tangentially follow the Flower framework while keeping the computational requirements to a minimum.\n",
    "\n",
    "The simplest and most common encoding for models is the mere transmission of model parameters as NumPy arrays instead of the stateful PyTorch models. The following functions allow for seamless conversions between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Oh2wjiNsVTZI"
   },
   "outputs": [],
   "source": [
    "def set_model_parameters(net: Module, parameters: NDArrays) -> Module:\n",
    "    \"\"\"Put a set of parameters into the model object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        net (Module): model object.\n",
    "        parameters (NDArrays): set of parameters to put into the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Module: updated model object.\n",
    "    \"\"\"\n",
    "    weights = parameters\n",
    "    params_dict = zip(net.state_dict().keys(), weights, strict=False)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(np.copy(v)) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_model_parameters(net: Module) -> NDArrays:\n",
    "    \"\"\"Get the current model parameters as NDArrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        net (Module): current model object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        NDArrays: set of parameters from the current model.\n",
    "    \"\"\"\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHJGcNa7FBUD"
   },
   "source": [
    "With them in place, making the client abstraction compatible with Flower requires only a bit of boilerplate such as allowing NumPy arrays to be received and sent instead of PyTorch models. To achieve this, we provide a model generator capable of creating a network and using the received parameters.\n",
    "\n",
    "To keep client objects light in the memory when not used by the Flower FL simulator, the model generator is only called as needed for either `fit` or `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SCqhfpnJobOZ"
   },
   "outputs": [],
   "source": [
    "class FlowerRayClient(flwr.client.NumPyClient):\n",
    "    \"\"\"Flower client for the FEMNIST dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        partition_dir: Path,\n",
    "        model_generator: Callable[[], Module],\n",
    "    ) -> None:\n",
    "        \"\"\"Init the client with its unique id and the folder to load data from.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            cid (int): Unique client id for a client used to map it to its data\n",
    "                partition\n",
    "            partition_dir (Path): The directory containing data for each\n",
    "                client/client id\n",
    "            model_generator (Callable[[], Module]): The model generator function\n",
    "        \"\"\"\n",
    "        self.cid = cid\n",
    "        log(INFO, \"cid: %s\", self.cid)\n",
    "        self.partition_dir = partition_dir\n",
    "        self.device = str(\n",
    "            torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.model_generator: Callable[[], Module] = model_generator\n",
    "        self.properties: dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "\n",
    "    def set_parameters(self, parameters: NDArrays) -> Module:\n",
    "        \"\"\"Load weights inside the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            parameters (NDArrays): set of weights to be loaded.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            [Module]: Network with new set of weights.\n",
    "        \"\"\"\n",
    "        net = self.model_generator()\n",
    "        return set_model_parameters(net, parameters)\n",
    "\n",
    "    def get_parameters(self, config: dict[str, Scalar]) -> NDArrays:\n",
    "        \"\"\"Return weights from a given model.\n",
    "\n",
    "        If no model is passed, then a local model is created.\n",
    "        This can be used to initialise a model in the\n",
    "        server.\n",
    "        The config param is not used but is mandatory in Flower.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            config (dict[int, Scalar]): dictionary containing configuration info.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            NDArrays: weights from the model.\n",
    "        \"\"\"\n",
    "        net = self.model_generator()\n",
    "        return get_model_parameters(net)\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: NDArrays, config: dict[str, Scalar]\n",
    "    ) -> tuple[NDArrays, int, dict]:\n",
    "        \"\"\"Receive and train a model on the local client data.\n",
    "\n",
    "        It uses parameters from the config dict\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (dict[str, Scalar]): dictionary describing the training parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[NDArrays, int, dict]: Returns the updated model, the size of the local\n",
    "                dataset and other metrics\n",
    "        \"\"\"\n",
    "        # Only create model right before training/testing\n",
    "        # To lower memory usage when idle\n",
    "        net = self.set_parameters(parameters)\n",
    "        net.to(self.device)\n",
    "\n",
    "        train_loader: DataLoader = self._create_data_loader(config, name=\"train\")\n",
    "        train_loss = self._train(net, train_loader=train_loader, config=config)\n",
    "        return get_model_parameters(net), len(train_loader), {\"train_loss\": train_loss}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: NDArrays, config: dict[str, Scalar]\n",
    "    ) -> tuple[float, int, dict]:\n",
    "        \"\"\"Receive and test a model on the local client data.\n",
    "\n",
    "        It uses parameters from the config dict\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (dict[str, Scalar]): dictionary describing the testing parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[float, int, dict]: Returns the loss accumulate during testing, the\n",
    "                size of the local dataset and other metrics such as accuracy\n",
    "        \"\"\"\n",
    "        net = self.set_parameters(parameters)\n",
    "        net.to(self.device)\n",
    "\n",
    "        test_loader: DataLoader = self._create_data_loader(config, name=\"test\")\n",
    "        loss, accuracy = self._test(net, test_loader=test_loader, config=config)\n",
    "        return loss, len(test_loader), {\"local_accuracy\": accuracy}\n",
    "\n",
    "    def _create_data_loader(self, config: dict[str, Scalar], name: str) -> DataLoader:\n",
    "        \"\"\"Create the data loader using the specified config parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            config (dict[str, Scalar]): dictionary containing dataloader and dataset\n",
    "                parameters\n",
    "            mode (str): Load the training or testing set for the client\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            DataLoader: A pytorch dataloader iterable for training/testing\n",
    "        \"\"\"\n",
    "        batch_size = int(config[\"batch_size\"])\n",
    "        num_workers = int(config[\"num_workers\"])\n",
    "        dataset = self._load_dataset(name)\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=(name == \"train\"),\n",
    "        )\n",
    "\n",
    "    def _load_dataset(self, name: str) -> Dataset:\n",
    "        full_file: Path = self.partition_dir / str(self.cid)\n",
    "        return load_femnist_dataset(\n",
    "            mapping=full_file,\n",
    "            name=name,\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "\n",
    "    def _train(\n",
    "        self, net: Module, train_loader: DataLoader, config: dict[str, Scalar]\n",
    "    ) -> float:\n",
    "        return train_femnist(\n",
    "            net=net,\n",
    "            train_loader=train_loader,\n",
    "            epochs=int(config[\"epochs\"]),\n",
    "            device=self.device,\n",
    "            optimizer=torch.optim.AdamW(\n",
    "                net.parameters(),\n",
    "                lr=float(config[\"client_learning_rate\"]),\n",
    "                weight_decay=float(config[\"weight_decay\"]),\n",
    "            ),\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=int(config[\"max_batches\"]),\n",
    "        )\n",
    "\n",
    "    def _test(\n",
    "        self, net: Module, test_loader: DataLoader, config: dict[str, Scalar]\n",
    "    ) -> tuple[float, float]:\n",
    "        return test_femnist(\n",
    "            net=net,\n",
    "            test_loader=test_loader,\n",
    "            device=self.device,\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=int(config[\"max_batches\"]),\n",
    "        )\n",
    "\n",
    "    def get_properties(self, config: dict[str, Scalar]) -> dict[str, Scalar]:\n",
    "        \"\"\"Return properties for this client.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            config (dict[str, Scalar]): Options to be used for selecting specific\n",
    "            properties.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            dict[str, Scalar]: Returned properties.\n",
    "        \"\"\"\n",
    "        return self.properties\n",
    "\n",
    "    def get_train_set_size(self) -> int:\n",
    "        \"\"\"Return the client train set size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: train set size of the client.\n",
    "        \"\"\"\n",
    "        return len(self._load_dataset(\"train\"))  # type: ignore[reportArgumentType]\n",
    "\n",
    "    def get_test_set_size(self) -> int:\n",
    "        \"\"\"Return the client test set size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: test set size of the client.\n",
    "        \"\"\"\n",
    "        return len(self._load_dataset(\"test\"))  # type: ignore[reportArgumentType]\n",
    "\n",
    "\n",
    "def fit_client_seeded(\n",
    "    client: FlowerRayClient,\n",
    "    params: NDArrays,\n",
    "    conf: dict[str, Any],\n",
    "    seed: Seeds = Seeds.DEFAULT,\n",
    "    **kwargs: Any,\n",
    ") -> tuple[NDArrays, int, dict]:\n",
    "    \"\"\"Wrap to always seed client training.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    return client.fit(params, conf, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HdLT_IBFBUD"
   },
   "source": [
    "The underlying FL simulator used by Flower is based on [Ray](https://www.ray.io/). It expects each client only to require a client ID for instantiation. Therefore, using the following generator function, we can determine the specific network used for FL together with the FEMNIST partition to which the `cid` refers.\n",
    "\n",
    "While we will not use `Ray` in this lab due to its heavyweight nature, we will keep all code API compatible with the default flower framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mwfh5JZmUcfd"
   },
   "outputs": [],
   "source": [
    "def get_flower_client_generator(\n",
    "    model_generator: Callable[[], Module],\n",
    "    partition_dir: Path,\n",
    "    mapping_fn: Callable[[int], int] | None = None,\n",
    ") -> Callable[[str], FlowerRayClient]:\n",
    "    \"\"\"Wrap the client instance generator.\n",
    "\n",
    "    This provides the client generator with a model generator function.\n",
    "    Also, the partition directory must be passed.\n",
    "    A mapping function could be used for filtering/ordering clients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "        mapping_fn (Optional[Callable[[int], int]]): function mapping sorted/filtered\n",
    "            ids to real cid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[str], FlowerRayClient]: client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerRayClient:\n",
    "        \"\"\"Create a single client instance given the client id `cid`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            cid (str): client id, Flower requires this to be of type str.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            FlowerRayClient: client instance.\n",
    "        \"\"\"\n",
    "        return FlowerRayClient(\n",
    "            cid=mapping_fn(int(cid)) if mapping_fn is not None else int(cid),\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "        )\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4rBKYJqU3bf"
   },
   "source": [
    "To ensure the Flower client behaves the same as our simple demo client, a simple test using the centralised partition we defined earlier should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "udMWNQ0xU8VD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:55,313 | 2290543087.py:21 | cid: 0\n"
     ]
    }
   ],
   "source": [
    "network_generator = get_network_generator()\n",
    "seed_net: Net = network_generator()\n",
    "seed_model_params: NDArrays = get_model_parameters(seed_net)\n",
    "\n",
    "centralized_flower_client_generator: Callable[[str], FlowerRayClient] = (\n",
    "    get_flower_client_generator(network_generator, centralized_partition)\n",
    ")\n",
    "centralized_flower_client = centralized_flower_client_generator(str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GJima3hoFBUD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:57,844 | 4024557794.py:20 | Train Metrics = {'train_loss': 0.11729335479438305}\n"
     ]
    }
   ],
   "source": [
    "centralized_train_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "# Train parameters on the centralised dataset\n",
    "trained_params, num_examples, train_metrics = fit_client_seeded(\n",
    "    centralized_flower_client, params=seed_model_params, conf=centralized_train_config\n",
    ")\n",
    "log(INFO, \"Train Metrics = %s\", train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EdzCbOf0wOWC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2329 [00:01<00:27, 80.77it/s]\n",
      "INFO flwr 2025-01-29 11:56:59,182 | 592755603.py:5 | Loss = 370.0846788883209; Test Metrics = {'local_accuracy': 0.059375}\n"
     ]
    }
   ],
   "source": [
    "# Test trained parameters on the centralised dataset\n",
    "loss, num_examples, test_metrics = centralized_flower_client.evaluate(\n",
    "    parameters=trained_params, config=test_config\n",
    ")\n",
    "log(INFO, \"Loss = %s; Test Metrics = %s\", loss, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXwkNqUED40K"
   },
   "source": [
    "## 6. FL with natural partitions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC0MPZAIM-JC"
   },
   "source": [
    "Given its naturally-partitioned nature, we can easily construct a realistic FL experiment by mapping clients one-to-one with the writers of the original symbols.\n",
    "\n",
    "To pursue this aim, we shall use the “naturally federated” partition instead of the \"centralised” one. We are then using the entire `train.csv` and `test.csv` contained in the subfolders of `client_data_mappings/fed_natural`. Each subfolder is named after the clients' ID in the dataset, i.e., from `0` to `3229`.\n",
    "\n",
    "To guarantee that each client has sufficient training data to participate meaningfully, a common practice is to set a lower bound on the number of samples a selected client is allowed to have. Generally, this threshold should be equivalent to at least one batch. We will now implement a function to sample clients from the federation that satisfies the abovementioned filter in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6QQv15XxXhvJ"
   },
   "outputs": [],
   "source": [
    "def sample_random_clients(\n",
    "    total_clients: int,\n",
    "    filter_less: int,\n",
    "    partition: Path,\n",
    "    seed: int | None = Seeds.DEFAULT,\n",
    ") -> Sequence[int]:\n",
    "    \"\"\"Sample randomly clients.\n",
    "\n",
    "    A filter on the client train set size is performed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        total_clients (int): total number of clients to sample.\n",
    "        filter_less (int): max number of train samples for which the client is\n",
    "            **discarded**.\n",
    "        partition (Path): path to the folder containing the partitioning.\n",
    "        seed (Optional[int], optional): seed for the random generator. Defaults to None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Sequence[int]: list of sample client ids as int.\n",
    "    \"\"\"\n",
    "    real_federated_cid_client_generator: Callable[[str], FlowerRayClient] = (\n",
    "        get_flower_client_generator(network_generator, federated_partition)\n",
    "    )\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    list_of_ids = []\n",
    "    while len(list_of_ids) < total_clients:\n",
    "        current_id = random.randint(0, 3229)\n",
    "        if (\n",
    "            real_federated_cid_client_generator(str(current_id)).get_train_set_size()\n",
    "            > filter_less\n",
    "        ):\n",
    "            list_of_ids.append(current_id)\n",
    "    return list_of_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "togXum333D1L"
   },
   "source": [
    "While FEMNIST has more than 3000 clients, our small-scale experiments will not require more than 100 at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e-s7oVYNXkwf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:59,194 | 2290543087.py:21 | cid: 2530\n",
      "INFO flwr 2025-01-29 11:56:59,196 | 2290543087.py:21 | cid: 2184\n",
      "INFO flwr 2025-01-29 11:56:59,197 | 2290543087.py:21 | cid: 2907\n",
      "INFO flwr 2025-01-29 11:56:59,198 | 2290543087.py:21 | cid: 1498\n",
      "INFO flwr 2025-01-29 11:56:59,199 | 2290543087.py:21 | cid: 2338\n",
      "INFO flwr 2025-01-29 11:56:59,200 | 2290543087.py:21 | cid: 2399\n",
      "INFO flwr 2025-01-29 11:56:59,201 | 2290543087.py:21 | cid: 2997\n",
      "INFO flwr 2025-01-29 11:56:59,202 | 2290543087.py:21 | cid: 678\n",
      "INFO flwr 2025-01-29 11:56:59,203 | 2290543087.py:21 | cid: 3175\n",
      "INFO flwr 2025-01-29 11:56:59,204 | 2290543087.py:21 | cid: 1363\n",
      "INFO flwr 2025-01-29 11:56:59,204 | 2290543087.py:21 | cid: 1571\n",
      "INFO flwr 2025-01-29 11:56:59,205 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:56:59,206 | 2290543087.py:21 | cid: 1473\n",
      "INFO flwr 2025-01-29 11:56:59,207 | 2290543087.py:21 | cid: 1260\n",
      "INFO flwr 2025-01-29 11:56:59,208 | 2290543087.py:21 | cid: 1603\n",
      "INFO flwr 2025-01-29 11:56:59,209 | 2290543087.py:21 | cid: 2855\n",
      "INFO flwr 2025-01-29 11:56:59,210 | 2290543087.py:21 | cid: 839\n",
      "INFO flwr 2025-01-29 11:56:59,211 | 2290543087.py:21 | cid: 3119\n",
      "INFO flwr 2025-01-29 11:56:59,212 | 2290543087.py:21 | cid: 2688\n",
      "INFO flwr 2025-01-29 11:56:59,212 | 2290543087.py:21 | cid: 1494\n",
      "INFO flwr 2025-01-29 11:56:59,213 | 2290543087.py:21 | cid: 447\n",
      "INFO flwr 2025-01-29 11:56:59,214 | 2290543087.py:21 | cid: 1742\n",
      "INFO flwr 2025-01-29 11:56:59,215 | 2290543087.py:21 | cid: 2601\n",
      "INFO flwr 2025-01-29 11:56:59,216 | 2290543087.py:21 | cid: 1633\n",
      "INFO flwr 2025-01-29 11:56:59,216 | 2290543087.py:21 | cid: 267\n",
      "INFO flwr 2025-01-29 11:56:59,217 | 2290543087.py:21 | cid: 2070\n",
      "INFO flwr 2025-01-29 11:56:59,218 | 2290543087.py:21 | cid: 2863\n",
      "INFO flwr 2025-01-29 11:56:59,219 | 2290543087.py:21 | cid: 2736\n",
      "INFO flwr 2025-01-29 11:56:59,220 | 2290543087.py:21 | cid: 1425\n",
      "INFO flwr 2025-01-29 11:56:59,221 | 2290543087.py:21 | cid: 1653\n",
      "INFO flwr 2025-01-29 11:56:59,222 | 2290543087.py:21 | cid: 1652\n",
      "INFO flwr 2025-01-29 11:56:59,222 | 2290543087.py:21 | cid: 3020\n",
      "INFO flwr 2025-01-29 11:56:59,223 | 2290543087.py:21 | cid: 1273\n",
      "INFO flwr 2025-01-29 11:56:59,224 | 2290543087.py:21 | cid: 2718\n",
      "INFO flwr 2025-01-29 11:56:59,225 | 2290543087.py:21 | cid: 73\n",
      "INFO flwr 2025-01-29 11:56:59,226 | 2290543087.py:21 | cid: 1446\n",
      "INFO flwr 2025-01-29 11:56:59,227 | 2290543087.py:21 | cid: 2434\n",
      "INFO flwr 2025-01-29 11:56:59,228 | 2290543087.py:21 | cid: 485\n",
      "INFO flwr 2025-01-29 11:56:59,228 | 2290543087.py:21 | cid: 1887\n",
      "INFO flwr 2025-01-29 11:56:59,229 | 2290543087.py:21 | cid: 1009\n",
      "INFO flwr 2025-01-29 11:56:59,230 | 2290543087.py:21 | cid: 701\n",
      "INFO flwr 2025-01-29 11:56:59,231 | 2290543087.py:21 | cid: 1285\n",
      "INFO flwr 2025-01-29 11:56:59,232 | 2290543087.py:21 | cid: 2782\n",
      "INFO flwr 2025-01-29 11:56:59,233 | 2290543087.py:21 | cid: 2828\n",
      "INFO flwr 2025-01-29 11:56:59,234 | 2290543087.py:21 | cid: 2476\n",
      "INFO flwr 2025-01-29 11:56:59,234 | 2290543087.py:21 | cid: 1872\n",
      "INFO flwr 2025-01-29 11:56:59,235 | 2290543087.py:21 | cid: 2471\n",
      "INFO flwr 2025-01-29 11:56:59,236 | 2290543087.py:21 | cid: 1084\n",
      "INFO flwr 2025-01-29 11:56:59,237 | 2290543087.py:21 | cid: 823\n",
      "INFO flwr 2025-01-29 11:56:59,243 | 2290543087.py:21 | cid: 2243\n",
      "INFO flwr 2025-01-29 11:56:59,248 | 2290543087.py:21 | cid: 275\n",
      "INFO flwr 2025-01-29 11:56:59,257 | 2290543087.py:21 | cid: 2614\n",
      "INFO flwr 2025-01-29 11:56:59,259 | 2290543087.py:21 | cid: 2152\n",
      "INFO flwr 2025-01-29 11:56:59,261 | 2290543087.py:21 | cid: 2534\n",
      "INFO flwr 2025-01-29 11:56:59,262 | 2290543087.py:21 | cid: 2364\n",
      "INFO flwr 2025-01-29 11:56:59,264 | 2290543087.py:21 | cid: 3168\n",
      "INFO flwr 2025-01-29 11:56:59,265 | 2290543087.py:21 | cid: 179\n",
      "INFO flwr 2025-01-29 11:56:59,266 | 2290543087.py:21 | cid: 295\n",
      "INFO flwr 2025-01-29 11:56:59,267 | 2290543087.py:21 | cid: 1668\n",
      "INFO flwr 2025-01-29 11:56:59,268 | 2290543087.py:21 | cid: 2674\n",
      "INFO flwr 2025-01-29 11:56:59,268 | 2290543087.py:21 | cid: 2538\n",
      "INFO flwr 2025-01-29 11:56:59,269 | 2290543087.py:21 | cid: 220\n",
      "INFO flwr 2025-01-29 11:56:59,270 | 2290543087.py:21 | cid: 2767\n",
      "INFO flwr 2025-01-29 11:56:59,271 | 2290543087.py:21 | cid: 70\n",
      "INFO flwr 2025-01-29 11:56:59,272 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:56:59,273 | 2290543087.py:21 | cid: 806\n",
      "INFO flwr 2025-01-29 11:56:59,274 | 2290543087.py:21 | cid: 807\n",
      "INFO flwr 2025-01-29 11:56:59,275 | 2290543087.py:21 | cid: 428\n",
      "INFO flwr 2025-01-29 11:56:59,276 | 2290543087.py:21 | cid: 1167\n",
      "INFO flwr 2025-01-29 11:56:59,276 | 2290543087.py:21 | cid: 805\n",
      "INFO flwr 2025-01-29 11:56:59,277 | 2290543087.py:21 | cid: 1852\n",
      "INFO flwr 2025-01-29 11:56:59,278 | 2290543087.py:21 | cid: 3068\n",
      "INFO flwr 2025-01-29 11:56:59,279 | 2290543087.py:21 | cid: 2329\n",
      "INFO flwr 2025-01-29 11:56:59,279 | 2290543087.py:21 | cid: 1287\n",
      "INFO flwr 2025-01-29 11:56:59,280 | 2290543087.py:21 | cid: 51\n",
      "INFO flwr 2025-01-29 11:56:59,281 | 2290543087.py:21 | cid: 2501\n",
      "INFO flwr 2025-01-29 11:56:59,282 | 2290543087.py:21 | cid: 1366\n",
      "INFO flwr 2025-01-29 11:56:59,283 | 2290543087.py:21 | cid: 1770\n",
      "INFO flwr 2025-01-29 11:56:59,284 | 2290543087.py:21 | cid: 2343\n",
      "INFO flwr 2025-01-29 11:56:59,285 | 2290543087.py:21 | cid: 937\n",
      "INFO flwr 2025-01-29 11:56:59,285 | 2290543087.py:21 | cid: 2251\n",
      "INFO flwr 2025-01-29 11:56:59,286 | 2290543087.py:21 | cid: 187\n",
      "INFO flwr 2025-01-29 11:56:59,287 | 2290543087.py:21 | cid: 3178\n",
      "INFO flwr 2025-01-29 11:56:59,288 | 2290543087.py:21 | cid: 2274\n",
      "INFO flwr 2025-01-29 11:56:59,289 | 2290543087.py:21 | cid: 2975\n",
      "INFO flwr 2025-01-29 11:56:59,290 | 2290543087.py:21 | cid: 2645\n",
      "INFO flwr 2025-01-29 11:56:59,291 | 2290543087.py:21 | cid: 1258\n",
      "INFO flwr 2025-01-29 11:56:59,291 | 2290543087.py:21 | cid: 875\n",
      "INFO flwr 2025-01-29 11:56:59,292 | 2290543087.py:21 | cid: 2504\n",
      "INFO flwr 2025-01-29 11:56:59,293 | 2290543087.py:21 | cid: 740\n",
      "INFO flwr 2025-01-29 11:56:59,294 | 2290543087.py:21 | cid: 2167\n",
      "INFO flwr 2025-01-29 11:56:59,295 | 2290543087.py:21 | cid: 2157\n",
      "INFO flwr 2025-01-29 11:56:59,295 | 2290543087.py:21 | cid: 2164\n",
      "INFO flwr 2025-01-29 11:56:59,296 | 2290543087.py:21 | cid: 757\n",
      "INFO flwr 2025-01-29 11:56:59,297 | 2290543087.py:21 | cid: 3175\n",
      "INFO flwr 2025-01-29 11:56:59,298 | 2290543087.py:21 | cid: 2714\n",
      "INFO flwr 2025-01-29 11:56:59,298 | 2290543087.py:21 | cid: 206\n",
      "INFO flwr 2025-01-29 11:56:59,299 | 2290543087.py:21 | cid: 3057\n",
      "INFO flwr 2025-01-29 11:56:59,300 | 2290543087.py:21 | cid: 2026\n",
      "INFO flwr 2025-01-29 11:56:59,301 | 2290543087.py:21 | cid: 2882\n"
     ]
    }
   ],
   "source": [
    "total_clients: int = 100\n",
    "list_of_ids = sample_random_clients(\n",
    "    total_clients, centralized_train_config[\"batch_size\"], federated_partition\n",
    ")\n",
    "\n",
    "federated_client_generator: Callable[[str], FlowerRayClient] = (\n",
    "    get_flower_client_generator(\n",
    "        network_generator, federated_partition, lambda seq_id: list_of_ids[seq_id]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUrsR0JJwZ5R"
   },
   "source": [
    "Now, to test that the newly partitioned clients can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1HzR2ZY7nhPm"
   },
   "outputs": [],
   "source": [
    "one_epoch_config: dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wyoErIaM3D1L"
   },
   "outputs": [],
   "source": [
    "num_clients = 4\n",
    "clients = random.sample(list(range(total_clients)), num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FC1QOYivn_PX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:59,316 | 2290543087.py:21 | cid: 3175\n",
      "INFO flwr 2025-01-29 11:56:59,398 | 2290543087.py:21 | cid: 51\n",
      "INFO flwr 2025-01-29 11:56:59,607 | 2290543087.py:21 | cid: 1668\n",
      "INFO flwr 2025-01-29 11:56:59,667 | 2290543087.py:21 | cid: 1498\n"
     ]
    }
   ],
   "source": [
    "trained_models = [\n",
    "    fit_client_seeded(\n",
    "        federated_client_generator(str(cid)), seed_model_params, one_epoch_config\n",
    "    )\n",
    "    for cid in clients\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ATLQPQ_fm9ph"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:59,755 | 756348695.py:3 | Metrics from trained models are: [[4, {'train_loss': 0.12445169873535633}], [11, {'train_loss': 0.12350663881410252}], [3, {'train_loss': 0.12321196496486664}], [4, {'train_loss': 0.12363967671990395}]]\n"
     ]
    }
   ],
   "source": [
    "trained_model_parameters = [model for model, *rest in trained_models]\n",
    "trained_model_metrics = [rest for _, *rest in trained_models]\n",
    "log(INFO, \"Metrics from trained models are: %s\", trained_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgg0G0i23D1L"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 5 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "If we index the weights of an ML model consecutively, with the assumed architecture being the same, we can visualize it as a single flattened 1-D vector. One, highly imperfect, metric for determining how similar two ML models are is cosine similarity. We can use this metric to compare the models our clients produce are.\n",
    "\n",
    "1. Write functions to:\n",
    "    - Flatten the `NDArrays` objects into a single 1-D vector.\n",
    "    - Compute the cosine similarity of two 1-D vectors based on their inner product and norms.\n",
    "2. Compute a similarity matrix between all client models and plot it.\n",
    "3. What happens to the similarity matrix if you increase the number of local epochs that clients train for to 5 from 1? Why do you think that is?\n",
    ">You may want to save the parameters calculated here separately from the deault ones as both sets will be needed in future questions.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 6 (Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "One reason that cosine similarity is an imperfect metric is because it considers weights from different layers to be equally important to the model in terms of its behavior and performance. We can thus create a slightly more detailed picture by tracking the cosine similarity across layers.\n",
    "\n",
    "1. Create a function which computes the pairwise cosine similarity between flattened model layers across clients, i.e. the cosine similarity between the flattened $l_i$ in `client_x` and the flattened $l_i$ in `client_y`.\n",
    "2. Plot the similarity of the models at each layer between the most similar client pair and the most disimilar client pair (as determined by the multi-epoch experiment) using the **one-epoch models**, how do the curves you observe relate to the general cosine similarity you computed for the flattened models.\n",
    "3. What happens to the curves if you increase the number of local epochs to 5? How does this behavior relate to what you observed in the previous exercise ?\n",
    "4. **Optional**: Do you think there is any connection to the underlying architecture? Why? Repeat the above experiments using an MLP such as the one provided by the  `get_network_generator_mlp` in `client_utils` and compare results against the CNN.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifI1FmOywjqL"
   },
   "source": [
    "## 7. Federated training with Flower\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfDcWWoc2B5q"
   },
   "source": [
    "The two basic blocks of synchronous server-client FL systems are:\n",
    "- A client with some local training method and data---i.e., SGD. This is what we have built thus far.\n",
    "- A server which coordinates training sends the federated model to clients at the start of each round and aggregates model updates at the end of each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk9W7CTgoUxS"
   },
   "source": [
    "\n",
    "![picture](https://drive.google.com/uc?id=1Db_Uys2QPFHW6cMranZ_QXo2vC0A_C-N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdcfcz_e2B5q"
   },
   "source": [
    "While the variety of client local training methods is as wide as ML, server training varies depending on which **aggregation algorithm** combines the model updates and forms the new federated model. Flower refers to the component that controls the aggregation and train/test configuration as a **strategy**. For our labs, strategy and aggregation algorithms will be used interchangeably.\n",
    "\n",
    "The strategy we shall use is [FedAvg](https://arxiv.org/abs/1602.05629):\n",
    "\n",
    "$G^{r+1} = G^{r} + \\eta \\left( \\sum_{k=1}^{m} p_k G^r_k \\right)$,\n",
    "\n",
    "where $G^{r+1}$ is the model for the next round, which is formed by applying a model update to the current round model $G^{r}$ weighted by the aggregation learning rate $\\eta$. The model update is constructed by a sum of client models $G^{r}_k$ where each client model is weighted by $p_k$. The weight factor is usually set to $p_k = \\tfrac{n_k}{N}$---the number of training examples held by said client divided by the example count of each client in the round $N = \\sum_{k=1}^{m} n_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7unJaupF-IaB"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 7 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "Averaging models is meant to provide us with a reasonable compromise between the desired models of all clients involved. Given what you already know about the similarity of client models, we can test if this is true.\n",
    "1. Write a function to compute the weighted average of two `NDArrays` objects while preserving the layer-structure. Use the FedAvg equation above with $p_k = \\tfrac{n_k}{N}$.\n",
    "2. Compute the average model of the client models above, both the one-epoch and multi-epoch ones.\n",
    "3. Add the averaged models to the cosine-similarity matrices built above, how does it relate to the client models?\n",
    "\n",
    "---\n",
    "\n",
    "**Question 8 (Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "\n",
    "1. Repeat the procedure above but for layer-wise cosine similarity and plot the cosine similarity of the two clients client most similar and most disimillar to the averaged model (as determined by the five-epoch condition) using the one-epoch models.\n",
    "2. How does the relationship change for the higher-epoch condition?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxsFjUN03D1M"
   },
   "source": [
    "The pieces necessary for starting an FL simulation are now in play; we need to arrange them to fit the Flower API. First, we shall require a separate federated evaluation function which can be called outside the context of a specific client. It will use the centralised test set to be as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OO7CqzaE3D1M"
   },
   "outputs": [],
   "source": [
    "def get_federated_evaluation_function(\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    model_generator: Callable[[], Module],\n",
    "    criterion: Module,\n",
    "    max_batches: int,\n",
    ") -> Callable[[int, NDArrays, dict[str, Any]], tuple[float, dict[str, Scalar]]]:\n",
    "    \"\"\"Wrap the external federated evaluation function.\n",
    "\n",
    "    It provides the external federated evaluation function with some\n",
    "    parameters for the dataloader, the model generator function, and\n",
    "    the criterion used in the evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        batch_size (int): batch size of the test set to use.\n",
    "        num_workers (int): correspond to `num_workers` param in the Dataloader object.\n",
    "        model_generator (Callable[[], Module]):  model generator function.\n",
    "        criterion (Module): PyTorch Module containing the criterion for evaluating the\n",
    "        model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[int, NDArrays, dict[str, Any]], tuple[float, dict[str, Scalar]]]:\n",
    "            external federated evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def federated_evaluation_function(\n",
    "        server_round: int,\n",
    "        parameters: NDArrays,\n",
    "        fed_eval_config: dict[\n",
    "            str, Any\n",
    "        ],  # mandatory argument, even if it's not being used\n",
    "    ) -> tuple[float, dict[str, Scalar]]:\n",
    "        \"\"\"Evaluate federated model on the server.\n",
    "\n",
    "        It uses the centralized val set for sake of simplicity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            server_round (int): current federated round.\n",
    "            parameters (NDArrays): current model parameters.\n",
    "            fed_eval_config (dict[str, Any]): mandatory argument in Flower, can contain\n",
    "                some configuration info\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[float, dict[str, Scalar]]: evaluation results\n",
    "        \"\"\"\n",
    "        device: str = get_device()\n",
    "        net: Module = set_model_parameters(model_generator(), parameters)\n",
    "        net.to(device)\n",
    "\n",
    "        full_file: Path = centralized_mapping\n",
    "        dataset: Dataset = load_femnist_dataset(data_dir, full_file, \"val\")\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        loss, acc = test_femnist(\n",
    "            net=net,\n",
    "            test_loader=valid_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            max_batches=max_batches,\n",
    "        )\n",
    "        return loss, {\"accuracy\": acc}\n",
    "\n",
    "    return federated_evaluation_function\n",
    "\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=test_config[\"batch_size\"],\n",
    "    num_workers=test_config[\"num_workers\"],\n",
    "    model_generator=network_generator,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=test_config[\"max_batches\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7D1cEkrB2B5r"
   },
   "outputs": [],
   "source": [
    "def aggregate_weighted_average(metrics: list[tuple[int, dict]]) -> dict:\n",
    "    \"\"\"Combine results from multiple clients following training or evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        metrics (list[tuple[int, dict]]): collected clients metrics\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: result dictionary containing the aggregate of the metrics passed.\n",
    "    \"\"\"\n",
    "    average_dict: dict = defaultdict(list)\n",
    "    total_examples: int = 0\n",
    "    for num_examples, metrics_dict in metrics:\n",
    "        for key, val in metrics_dict.items():\n",
    "            if isinstance(val, numbers.Number):\n",
    "                average_dict[key].append((num_examples, val))\n",
    "        total_examples += num_examples\n",
    "    return {\n",
    "        key: {\n",
    "            \"avg\": float(\n",
    "                sum([num_examples * metric for num_examples, metric in val])\n",
    "                / float(total_examples)\n",
    "            ),\n",
    "            \"all\": val,\n",
    "        }\n",
    "        for key, val in average_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-sxyPBQ0zSoD"
   },
   "outputs": [],
   "source": [
    "# Federated configuration dictionary\n",
    "federated_train_config: dict[str, Any] = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLIfqk0xhDj"
   },
   "source": [
    "The only challenge left is the FL simulation itself. In `Flower`, a `Server` object handles this for us by using `Ray` and spawning many heavyweight worker process.\n",
    "\n",
    "Given the limited-resource scenario in which we find ourselves, we provide you with a slightly modified simulation function which uses a simple thread pool. Feel free to swap it out for the original simulation or replace it with your own implementation if so inclined.\n",
    "\n",
    "> The server we use is not the default `Flower` server as it returns the model parameters from every single round in a `(round, NDArrays)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1Fs5gtK-3D1M"
   },
   "outputs": [],
   "source": [
    "def start_seeded_simulation(\n",
    "    client_fn: Callable[[str], Client],\n",
    "    num_clients: int,\n",
    "    config: ServerConfig,\n",
    "    strategy: Strategy,\n",
    "    name: str,\n",
    "    return_all_parameters: bool = False,\n",
    "    seed: int = Seeds.DEFAULT,\n",
    "    iteration: int = 0,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Wrap to seed client selection.\"\"\"\n",
    "    np.random.seed(seed ^ iteration)\n",
    "    torch.manual_seed(seed ^ iteration)\n",
    "    random.seed(seed ^ iteration)\n",
    "    parameter_list, hist = flwr.simulation.start_simulation_no_ray(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        client_resources={},\n",
    "        config=config,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    save_history(home_dir, hist, name)\n",
    "    return parameter_list, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JkqplxMXLm8K"
   },
   "outputs": [],
   "source": [
    "num_rounds = 10\n",
    "\n",
    "num_total_clients = 20\n",
    "\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 5\n",
    "\n",
    "initial_parameters = ndarrays_to_parameters(seed_model_params)\n",
    "\n",
    "\n",
    "def run_simulation(\n",
    "    # How long the FL process runs for:\n",
    "    num_rounds: int = num_rounds,\n",
    "    # Number of clients available\n",
    "    num_total_clients: int = num_total_clients,\n",
    "    # Number of clients used for train/eval\n",
    "    num_clients_per_round: int = num_clients_per_round,\n",
    "    num_evaluate_clients: int = num_evaluate_clients,\n",
    "    # If less clients are overall available stop FL\n",
    "    min_available_clients: int = num_total_clients,\n",
    "    # If less clients are available for fit/eval stop FL\n",
    "    min_fit_clients: int = num_clients_per_round,\n",
    "    min_evaluate_clients: int = num_evaluate_clients,\n",
    "    # Function to test the federated model performance\n",
    "    # external to a client instantiation\n",
    "    evaluate_fn: (\n",
    "        Callable[\n",
    "            [int, NDArrays, dict[str, Scalar]],\n",
    "            tuple[float, dict[str, Scalar]] | None,\n",
    "        ]\n",
    "        | None\n",
    "    ) = federated_evaluation_function,\n",
    "    # Functions to generate a config for client fit/evaluate\n",
    "    # by-default the same config is shallow-copied to all clients in Flower\n",
    "    # this version simply uses the configs defined above\n",
    "    on_fit_config_fn: Callable[\n",
    "        [int], dict[str, Scalar]\n",
    "    ] = lambda _x: federated_train_config,\n",
    "    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] = lambda _x: test_config,\n",
    "    # The \"Parameters\" type is merely a more packed version\n",
    "    # of numpy array lists, used internally by Flower\n",
    "    initial_parameters: Parameters = initial_parameters,\n",
    "    # If this is set to True, aggregation will work even if some clients fail\n",
    "    accept_failures: bool = False,\n",
    "    # How to combine the metrics dictionary returned by all clients for fit/eval\n",
    "    fit_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
    "    federated_client_generator: Callable[\n",
    "        [str], flwr.client.NumPyClient\n",
    "    ] = federated_client_generator,\n",
    "    # Aggregation learning rate for FedAvg\n",
    "    server_learning_rate: float = 1.0,\n",
    "    server_momentum: float = 0.0,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Run a federated simulation using Flower.\"\"\"\n",
    "    log(INFO, \"FL will execute for %s rounds\", num_rounds)\n",
    "\n",
    "    # Percentage of clients used for train/eval\n",
    "    fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "    fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_evaluate,\n",
    "        min_fit_clients=min_fit_clients,\n",
    "        min_evaluate_clients=min_evaluate_clients,\n",
    "        min_available_clients=min_available_clients,\n",
    "        on_fit_config_fn=on_fit_config_fn,\n",
    "        on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "        evaluate_fn=evaluate_fn,\n",
    "        initial_parameters=initial_parameters,\n",
    "        accept_failures=accept_failures,\n",
    "        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
    "        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "    )\n",
    "    # resetting the seed for the random selection of clients\n",
    "    # this way the list of clients trained is guaranteed to be always the same\n",
    "\n",
    "    cfg = ServerConfig(num_rounds)\n",
    "\n",
    "    def simulator_client_generator(cid: str) -> Client:\n",
    "        return federated_client_generator(cid).to_client()\n",
    "\n",
    "    parameters_for_each_round, hist = start_seeded_simulation(\n",
    "        client_fn=simulator_client_generator,\n",
    "        num_clients=num_total_clients,\n",
    "        config=cfg,\n",
    "        strategy=strategy,\n",
    "        name=\"fedavg\",\n",
    "        return_all_parameters=True,\n",
    "        seed=Seeds.DEFAULT,\n",
    "    )\n",
    "    return parameters_for_each_round, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "i4HpswKXbDup"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:56:59,809 | 857703202.py:56 | FL will execute for 10 rounds\n",
      "INFO flwr 2025-01-29 11:56:59,811 | app.py:149 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
      "INFO flwr 2025-01-29 11:56:59,812 | server_returns_parameters.py:81 | Initializing global parameters\n",
      "INFO flwr 2025-01-29 11:56:59,813 | server_returns_parameters.py:273 | Using initial parameters provided by strategy\n",
      "INFO flwr 2025-01-29 11:56:59,814 | server_returns_parameters.py:84 | Evaluating initial parameters\n",
      " 11%|█         | 100/891 [00:03<00:23, 33.26it/s]\n",
      "INFO flwr 2025-01-29 11:57:02,901 | server_returns_parameters.py:87 | initial parameters (loss, other metrics): 413.6842656135559, {'accuracy': 0.0065625}\n",
      "INFO flwr 2025-01-29 11:57:02,901 | server_returns_parameters.py:97 | FL starting\n",
      "DEBUG flwr 2025-01-29 11:57:02,902 | server_returns_parameters.py:223 | fit_round 1: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:02,903 | 2290543087.py:21 | cid: 1494\n",
      "INFO flwr 2025-01-29 11:57:02,903 | 2290543087.py:21 | cid: 3119\n",
      "INFO flwr 2025-01-29 11:57:02,904 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:57:02,904 | 2290543087.py:21 | cid: 2399\n",
      "INFO flwr 2025-01-29 11:57:02,905 | 2290543087.py:21 | cid: 1571\n",
      "DEBUG flwr 2025-01-29 11:57:03,949 | server_returns_parameters.py:237 | fit_round 1 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:04, 162.01it/s]\n",
      "INFO flwr 2025-01-29 11:57:04,588 | server_returns_parameters.py:120 | fit progress: (1, 367.4149968624115, {'accuracy': 0.074375}, 1.685973666026257)\n",
      "INFO flwr 2025-01-29 11:57:04,588 | server_returns_parameters.py:171 | evaluate_round 1: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:04,588 | server_returns_parameters.py:223 | fit_round 2: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:04,589 | 2290543087.py:21 | cid: 1473\n",
      "INFO flwr 2025-01-29 11:57:04,589 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:57:04,590 | 2290543087.py:21 | cid: 1363\n",
      "INFO flwr 2025-01-29 11:57:04,590 | 2290543087.py:21 | cid: 1494\n",
      "INFO flwr 2025-01-29 11:57:04,590 | 2290543087.py:21 | cid: 2997\n",
      "DEBUG flwr 2025-01-29 11:57:05,564 | server_returns_parameters.py:237 | fit_round 2 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:04, 158.62it/s]\n",
      "INFO flwr 2025-01-29 11:57:06,215 | server_returns_parameters.py:120 | fit progress: (2, 341.522757768631, {'accuracy': 0.079375}, 3.313573958002962)\n",
      "INFO flwr 2025-01-29 11:57:06,216 | server_returns_parameters.py:171 | evaluate_round 2: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:06,216 | server_returns_parameters.py:223 | fit_round 3: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:06,217 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:57:06,217 | 2290543087.py:21 | cid: 1498\n",
      "INFO flwr 2025-01-29 11:57:06,218 | 2290543087.py:21 | cid: 1260\n",
      "INFO flwr 2025-01-29 11:57:06,218 | 2290543087.py:21 | cid: 1473\n",
      "INFO flwr 2025-01-29 11:57:06,218 | 2290543087.py:21 | cid: 2907\n",
      "DEBUG flwr 2025-01-29 11:57:07,429 | server_returns_parameters.py:237 | fit_round 3 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 142.48it/s]\n",
      "INFO flwr 2025-01-29 11:57:08,153 | server_returns_parameters.py:120 | fit progress: (3, 345.6187195777893, {'accuracy': 0.069375}, 5.251444125024136)\n",
      "INFO flwr 2025-01-29 11:57:08,154 | server_returns_parameters.py:171 | evaluate_round 3: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:08,154 | server_returns_parameters.py:223 | fit_round 4: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:08,155 | 2290543087.py:21 | cid: 839\n",
      "INFO flwr 2025-01-29 11:57:08,155 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:57:08,155 | 2290543087.py:21 | cid: 1473\n",
      "INFO flwr 2025-01-29 11:57:08,156 | 2290543087.py:21 | cid: 3119\n",
      "INFO flwr 2025-01-29 11:57:08,156 | 2290543087.py:21 | cid: 1363\n",
      "DEBUG flwr 2025-01-29 11:57:09,173 | server_returns_parameters.py:237 | fit_round 4 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 150.39it/s]\n",
      "INFO flwr 2025-01-29 11:57:09,860 | server_returns_parameters.py:120 | fit progress: (4, 337.89319705963135, {'accuracy': 0.07375}, 6.957881374983117)\n",
      "INFO flwr 2025-01-29 11:57:09,860 | server_returns_parameters.py:171 | evaluate_round 4: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:09,860 | server_returns_parameters.py:223 | fit_round 5: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:09,861 | 2290543087.py:21 | cid: 2530\n",
      "INFO flwr 2025-01-29 11:57:09,862 | 2290543087.py:21 | cid: 2600\n",
      "INFO flwr 2025-01-29 11:57:09,862 | 2290543087.py:21 | cid: 1498\n",
      "INFO flwr 2025-01-29 11:57:09,863 | 2290543087.py:21 | cid: 1603\n",
      "INFO flwr 2025-01-29 11:57:09,863 | 2290543087.py:21 | cid: 678\n",
      "DEBUG flwr 2025-01-29 11:57:10,922 | server_returns_parameters.py:237 | fit_round 5 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 156.46it/s]\n",
      "INFO flwr 2025-01-29 11:57:11,582 | server_returns_parameters.py:120 | fit progress: (5, 330.23247361183167, {'accuracy': 0.1134375}, 8.68037329101935)\n",
      "INFO flwr 2025-01-29 11:57:11,583 | server_returns_parameters.py:171 | evaluate_round 5: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:11,583 | server_returns_parameters.py:223 | fit_round 6: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:11,584 | 2290543087.py:21 | cid: 2399\n",
      "INFO flwr 2025-01-29 11:57:11,584 | 2290543087.py:21 | cid: 1571\n",
      "INFO flwr 2025-01-29 11:57:11,584 | 2290543087.py:21 | cid: 1603\n",
      "INFO flwr 2025-01-29 11:57:11,584 | 2290543087.py:21 | cid: 3175\n",
      "INFO flwr 2025-01-29 11:57:11,585 | 2290543087.py:21 | cid: 2997\n",
      "DEBUG flwr 2025-01-29 11:57:12,520 | server_returns_parameters.py:237 | fit_round 6 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 155.60it/s]\n",
      "INFO flwr 2025-01-29 11:57:13,184 | server_returns_parameters.py:120 | fit progress: (6, 318.58369493484497, {'accuracy': 0.1615625}, 10.282633708033245)\n",
      "INFO flwr 2025-01-29 11:57:13,185 | server_returns_parameters.py:171 | evaluate_round 6: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:13,185 | server_returns_parameters.py:223 | fit_round 7: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:13,186 | 2290543087.py:21 | cid: 3119\n",
      "INFO flwr 2025-01-29 11:57:13,186 | 2290543087.py:21 | cid: 2907\n",
      "INFO flwr 2025-01-29 11:57:13,186 | 2290543087.py:21 | cid: 839\n",
      "INFO flwr 2025-01-29 11:57:13,186 | 2290543087.py:21 | cid: 2184\n",
      "INFO flwr 2025-01-29 11:57:13,187 | 2290543087.py:21 | cid: 2688\n",
      "DEBUG flwr 2025-01-29 11:57:14,181 | server_returns_parameters.py:237 | fit_round 7 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 148.43it/s]\n",
      "INFO flwr 2025-01-29 11:57:14,877 | server_returns_parameters.py:120 | fit progress: (7, 296.8571846485138, {'accuracy': 0.240625}, 11.975289207999595)\n",
      "INFO flwr 2025-01-29 11:57:14,877 | server_returns_parameters.py:171 | evaluate_round 7: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:14,878 | server_returns_parameters.py:223 | fit_round 8: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:14,879 | 2290543087.py:21 | cid: 1260\n",
      "INFO flwr 2025-01-29 11:57:14,879 | 2290543087.py:21 | cid: 2184\n",
      "INFO flwr 2025-01-29 11:57:14,879 | 2290543087.py:21 | cid: 2530\n",
      "INFO flwr 2025-01-29 11:57:14,879 | 2290543087.py:21 | cid: 2997\n",
      "INFO flwr 2025-01-29 11:57:14,880 | 2290543087.py:21 | cid: 839\n",
      "DEBUG flwr 2025-01-29 11:57:16,170 | server_returns_parameters.py:237 | fit_round 8 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 149.96it/s]\n",
      "INFO flwr 2025-01-29 11:57:16,858 | server_returns_parameters.py:120 | fit progress: (8, 276.13240098953247, {'accuracy': 0.281875}, 13.956631499982905)\n",
      "INFO flwr 2025-01-29 11:57:16,859 | server_returns_parameters.py:171 | evaluate_round 8: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:16,859 | server_returns_parameters.py:223 | fit_round 9: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:16,860 | 2290543087.py:21 | cid: 1498\n",
      "INFO flwr 2025-01-29 11:57:16,860 | 2290543087.py:21 | cid: 1363\n",
      "INFO flwr 2025-01-29 11:57:16,861 | 2290543087.py:21 | cid: 2997\n",
      "INFO flwr 2025-01-29 11:57:16,861 | 2290543087.py:21 | cid: 1603\n",
      "INFO flwr 2025-01-29 11:57:16,862 | 2290543087.py:21 | cid: 1571\n",
      "DEBUG flwr 2025-01-29 11:57:17,926 | server_returns_parameters.py:237 | fit_round 9 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:05, 149.72it/s]\n",
      "INFO flwr 2025-01-29 11:57:18,617 | server_returns_parameters.py:120 | fit progress: (9, 280.8420171737671, {'accuracy': 0.325625}, 15.714853833022062)\n",
      "INFO flwr 2025-01-29 11:57:18,617 | server_returns_parameters.py:171 | evaluate_round 9: no clients selected, cancel\n",
      "DEBUG flwr 2025-01-29 11:57:18,617 | server_returns_parameters.py:223 | fit_round 10: strategy sampled 5 clients (out of 20)\n",
      "INFO flwr 2025-01-29 11:57:18,618 | 2290543087.py:21 | cid: 2530\n",
      "INFO flwr 2025-01-29 11:57:18,618 | 2290543087.py:21 | cid: 1571\n",
      "INFO flwr 2025-01-29 11:57:18,619 | 2290543087.py:21 | cid: 1260\n",
      "INFO flwr 2025-01-29 11:57:18,619 | 2290543087.py:21 | cid: 678\n",
      "INFO flwr 2025-01-29 11:57:18,620 | 2290543087.py:21 | cid: 2184\n",
      "DEBUG flwr 2025-01-29 11:57:20,024 | server_returns_parameters.py:237 | fit_round 10 received 5 results and 0 failures\n",
      " 11%|█         | 100/891 [00:00<00:06, 120.05it/s]\n",
      "INFO flwr 2025-01-29 11:57:20,880 | server_returns_parameters.py:120 | fit progress: (10, 254.65470218658447, {'accuracy': 0.3596875}, 17.977846583002247)\n",
      "INFO flwr 2025-01-29 11:57:20,880 | server_returns_parameters.py:171 | evaluate_round 10: no clients selected, cancel\n",
      "INFO flwr 2025-01-29 11:57:20,880 | server_returns_parameters.py:150 | FL finished in 17.978554749977775\n",
      "INFO flwr 2025-01-29 11:57:20,881 | app.py:250 | app_fit: losses_distributed []\n",
      "INFO flwr 2025-01-29 11:57:20,882 | app.py:251 | app_fit: metrics_distributed_fit {'train_loss': [(1, {'avg': 0.10525932198479063, 'all': [(5, 0.108098366856575), (4, 0.10343646071851254), (4, 0.10785755328834057), (4, 0.10154183954000473), (4, 0.10465262830257416)]}), (2, {'avg': 0.1041580896292414, 'all': [(5, 0.10419846326112747), (4, 0.10214669443666935), (3, 0.10661039253075917), (4, 0.10486570186913013), (5, 0.10368936061859131)]}), (3, {'avg': 0.10727026344587405, 'all': [(5, 0.10442855060100556), (8, 0.1151179876178503), (4, 0.10305161215364933), (3, 0.10228109608093898), (4, 0.10308748297393322)]}), (4, {'avg': 0.09809232093393802, 'all': [(4, 0.0929589532315731), (3, 0.09456777324279149), (4, 0.09973684325814247), (5, 0.09973435550928116), (4, 0.10217203386127949)]}), (5, {'avg': 0.09655840921661128, 'all': [(4, 0.09101406298577785), (5, 0.09005719572305679), (5, 0.09096416234970092), (6, 0.11154403537511826), (3, 0.0941387191414833)]}), (6, {'avg': 0.08667639903724193, 'all': [(4, 0.08796499110758305), (3, 0.0855311652024587), (4, 0.07979452796280384), (5, 0.08075605630874634), (4, 0.10052903182804585)]}), (7, {'avg': 0.0785969126969576, 'all': [(4, 0.07364962808787823), (4, 0.08888215944170952), (4, 0.08096610009670258), (4, 0.07459956035017967), (4, 0.07488711550831795)]}), (8, {'avg': 0.07420038302930501, 'all': [(4, 0.08494097553193569), (5, 0.0628044381737709), (5, 0.058216994255781175), (4, 0.07306125946342945), (8, 0.08651173207908869)]}), (9, {'avg': 0.0549848698079586, 'all': [(4, 0.05721748527139425), (4, 0.05759857315570116), (5, 0.04614712372422218), (4, 0.058469478972256184), (3, 0.05860654264688492)]}), (10, {'avg': 0.0613046462337176, 'all': [(5, 0.04767634272575379), (4, 0.06991254538297653), (8, 0.0650162361562252), (4, 0.046585059724748135), (6, 0.071787237500151)]})]}\n",
      "INFO flwr 2025-01-29 11:57:20,882 | app.py:252 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2025-01-29 11:57:20,882 | app.py:253 | app_fit: losses_centralized [(0, 413.6842656135559), (1, 367.4149968624115), (2, 341.522757768631), (3, 345.6187195777893), (4, 337.89319705963135), (5, 330.23247361183167), (6, 318.58369493484497), (7, 296.8571846485138), (8, 276.13240098953247), (9, 280.8420171737671), (10, 254.65470218658447)]\n",
      "INFO flwr 2025-01-29 11:57:20,883 | app.py:254 | app_fit: metrics_centralized {'accuracy': [(0, 0.0065625), (1, 0.074375), (2, 0.079375), (3, 0.069375), (4, 0.07375), (5, 0.1134375), (6, 0.1615625), (7, 0.240625), (8, 0.281875), (9, 0.325625), (10, 0.3596875)]}\n"
     ]
    }
   ],
   "source": [
    "parameters_for_each_round, hist = run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1CMXI3Uz3D1M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2025-01-29 11:57:20,891 | 1532827960.py:1 | Size of the list with the model parameters: 11\n"
     ]
    }
   ],
   "source": [
    "log(\n",
    "    INFO,\n",
    "    \"Size of the list with the model parameters: %s\",\n",
    "    len(parameters_for_each_round),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY9ELp7NuXEE"
   },
   "source": [
    "---\n",
    "\n",
    "**Question 9 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You must provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "Now that an entire FL experiment can be trained, it is worth considering the relationship between the FL model across rounds. If we consider the final model $\\theta^N$ obtained after $N$ rounds to be the optimal model $\\theta^*$, we can ask how optimization step contributed to reaching $\\theta^*$.\n",
    "\n",
    "1. Run an FL simulation for at-least 50 rounds.\n",
    "2. Build a function which reconstitutes the update that must have been applied to model $\\theta^t$ in order to obtain $\\theta^{t+1}$, assume the `server_learning_rate` was 1.0. We shall call this update $g_t$ and treat it as a pseudo-gradient.\n",
    "3. For each round plot the cosine similarity between $g_t$ and the direction of improvement towards $\\theta^{*}$ calculated as $\\theta^{*} - \\theta_t$.\n",
    "4. What do you observe from the plots? Does every update point in the direction of the final model? If not, why do you think that is?\n",
    "\n",
    "---\n",
    "\n",
    "**Question 10 (Part III/MPhil ✅):**\n",
    "\n",
    "(You must provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    " and then for\n",
    "\n",
    "1.   Read about server momentum in FL [here](https://arxiv.org/abs/2003.00295) and repeat the previous experiments using `server_learning_rate=1.0` and `server_momentum=0.9`. Discuss how the momentum impacts the direction of optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WfWeMqLQ0oT"
   },
   "source": [
    "(c) 2024 Alexandru-Andrei Iacob, Lorenzo Sani"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "labs-MC7Y7X2c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
